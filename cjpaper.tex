\documentclass[useAMS,usenatbib]{mn2e}

%%%%% AUTHORS - PLACE YOUR OWN MACROS HERE %%%%%
\newcommand{\conj}[1]{\overline{#1}}
\newcounter{NameOfTheNewCounter}
\setcounter{NameOfTheNewCounter}{1}
\newtheorem{theorem}{Theorem}[NameOfTheNewCounter]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{conjecture}[theorem]{Conjecture}
\usepackage{graphicx}
\usepackage{amsfonts}
\newcommand{\Gcal}{\bmath{\mathcal{G}}}
\newcommand{\Rcal}{\bmath{\mathcal{R}}}
\newcommand{\Mcal}{\bmath{\mathcal{M}}}
\newcommand{\Fcal}{\bmath{\mathcal{F}}}

\newcommand{\COMPLEX}{\mathbb{C}}
\newcommand{\REAL}{\mathbb{R}}
\newcommand{\II}{\mathbb{I}}

\newcommand{\zz}{\bmath{z}}

\newcommand{\mat}[1]{{\bmath{#1}}}

\newcommand{\JJ}{\mat{J}} % \bmath{\mathcal{J}}}
\newcommand{\DD}{\mat{D}}
\newcommand{\MM}{\mat{M}}
\newcommand{\RR}{\mat{R}}
\newcommand{\LL}{\mat{L}}
\newcommand{\RHO}{\mat{\Rho}}
\newcommand{\GG}{\mat{G}}
\newcommand{\JHJ}{\JJ^H\JJ} % \bmath{\mathcal{J}}}

\newcommand{\aaps}{A\&AS}
\newcommand{\aap}{A\&A}
\newcommand{\mnras}{MNRAS}
\newcommand{\nat}{Nature}
\newcommand{\physrep}{Phys. Rep.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title[Complex optimization for radio interferometeric calibration]{Complex optimization for radio interferometeric calibration}
\author[N. Bourbaki]{N. Bourbaki$^1$\thanks{E-mail: nbourbaki@pantheon.fr}\\ 
$^1$Institute For The Advancement Of Complex Chicken Recipes}
\begin{document}

\date{in original form 2014 February 2}

\pagerange{\pageref{firstpage}--\pageref{lastpage}} \pubyear{2014}

\maketitle

\label{firstpage}

\begin{abstract}
\end{abstract}

\begin{keywords}
Instrumentation: interferometers, Methods: analytical, Methods: numerical, Techniques: interferometric
\end{keywords}

\section{Introduction}

Chicken chicken chicken. Chicken chicken? Chicken\footnote{Chicken chicken!} chicken, chicken (Chicken \& Chicken 2014).

\section{Complex optimization \& Wirtinger calculus}

The traditional approach to optimizing a function of $n$ complex variables $f(\zz),$ $\zz\in\COMPLEX^n$ is
to treat the real and imaginary parts $\zz=\bmath{x}+i\bmath{y}$ independently, turning $f$ into a function
of $2n$ real variables $f(\bmath{x},\bmath{y})$, and the problem into an optimization over $\REAL^{2n}$.

\citet{ComplexOpt} have developed an alternative approach to the problem based on Wirtinger calculus. The central idea
of Wirtinger calculus is to treat $\zz$ and $\bar{\zz}$ as independent variables, and optimize $f(\zz,\bar{\zz})$
using the Wirtinger derivatives 

\[
\frac{\partial}{\partial z} = \frac{1}{2}\left ( \frac{\partial}{\partial x} - i\frac{\partial}{\partial y} \right),~~
\frac{\partial}{\partial \bar{z}} = \frac{1}{2}\left ( \frac{\partial}{\partial x} + i\frac{\partial}{\partial y} \right),
\]
where $z=x+iy$. It is easy to see that  
\[
\frac{\partial \bar z}{\partial z} = 
\frac{\partial z}{\partial \bar z} = 0,
\]
i.e. that $\bar z$ ($z$) is treated as constant when taking the derivative with respect to $z$ ($\bar z$). From this 
it is straightforward to define the \emph{complex gradient} operator 

\[
\frac{\partial}{\partial^C \zz} = \left [ \frac{\partial}{\partial \zz} \frac{\partial}{\partial \bar{\zz}} \right ] = \left [ \frac{\partial}{\partial z_1} \cdots \frac{\partial}{\partial z_n}
\frac{\partial}{\partial \bar{z}_1} \cdots \frac{\partial}{\partial \bar{z}_n} \right ],
\]

from which definitions of the complex Jacobian and complex Hessians naturally follow. The authors then show 
that various optimization techniques developed for real functions can be reformulated using 
complex Jacobians and Hessians, and applied to the complex optimization problem. Of particular interest to 
us, they generalize the Levenberg-Marquardt method for solving the the non-linear least squares (LS) problem

\begin{equation}
\label{eq:LSmin}
\min_{\bmath{z}} ||\bmath{r}(\bmath{z},\bmath{\bar z})||_F,
\end{equation}

where $\bmath{r}$ has values in $\COMPLEX^m$, and $||\cdot||_F$ is the Frobenius norm. This is done as follows.
Let $\zz_k$ be the parameter vector at step $k$. Then, define

\newcommand{\Matrix}[2]{\left [ \begin{array}{@{}#1@{}}#2\end{array} \right ]}
\newcommand{\Stack}[1]{\begin{array}{@{}c@{}}#1\end{array}}

\begin{equation}
\label{eq:Jk}
\JJ_k=\frac{\partial \bmath{r}}{\partial \zz} (\zz_k,\bar{\zz}_k), ~\JJ_{\bar{k}}=\frac{\partial \bmath{r}}{\partial \bar{\zz}}(\zz_k,\bar{\zz}_k),~\bmath{r}_k=\Matrix{c}{\bmath{r}(\zz_k,\bar{\zz}_k)\\\bar{\bmath{r}}(\zz_k,\bar{\zz}_k)}
\end{equation}

We'll call the $m\times n$ matrices $\JJ_k$ and $\JJ_{\bar{k}}$ the \emph{partial} and \emph{partial conjugate Jacobian}, respectively, 
and the $2m$-vector $\bmath{r}_k$ the \emph{augmented residual vector}. The \emph{complex Jacobian} 
of the cost function $||\bmath{r}(\bmath{z},\bmath{\bar z})||^2$ can then be written (in block matrix form) as

\begin{equation}
\label{eq:JJ}
\JJ = \Matrix{cc}{J_k & J_{\bar{k}} \\ \bar{J}_{\bar{k}} & \bar{J}_k },
\end{equation}

Note that this is a $2m \times 2n$ matrix. The LM parameter update step is then defined as

\begin{equation}
\label{eq:LM}
\Matrix{c}{\delta \zz \\ \delta \bar{\zz}} = -(\JJ^H \JJ + \lambda\II)^{-1}\JJ^H \bmath{r}_k,
\end{equation}

where $\II$ is the identity matrix, and $\lambda$ is the LM damping parameter. When $\lambda=0$ this becomes 
equivalent to the Newton-Gauss (NG) method; with $\lambda\to\infty$ this corresponds to steepest descent with ever smaller steps.

An alternative version of LM is formulated as

\begin{equation}
\label{eq:LM1}
\Matrix{c}{\delta \zz \\ \delta \bar{\zz}} = -(\JJ^H \JJ + \lambda \DD)^{-1}\JJ^H \bmath{r}_k,
\end{equation}

where $\DD$ is simply the diagonalized version of $\JJ^H\JJ$.

\citet{ComplexOpt} show that Eq.~\ref{eq:LM} yields exactly the same system of LS equations as would have 
been produced had we treated $\bmath{r}$ as a function of real and imaginary parts $\bmath{r}(\bmath{x},\bmath{y})$, 
and taken ordinary derivatives in $\REAL^{2n}$. However, the complex Jacobian may be easier and more elegant 
to derive analytically, as we'll see below in the case of radio interferometric calibration.

\section{Unpolarized direction independent calibration}

Let us first consider the simplest case, that of unpolarized direction-independent calibration. Consider an interferometer
array of $N$ antennas measuring $M=N(N-1)/2$ pairwise visibilities. Each antenna pair $pq$ ($1\leq p<q\leq N$) 
measures the visibility

\begin{equation}
\label{eq:RIME:unpol}
g_p m_{pq} \bar{g}_q + n_{pq},
\end{equation}

where $m_{pq}$ is the (assumed known) sky coherency, $g_p$ is the (unknown) complex gain parameter 
associated with antenna $p$, and $n_{pq}$ is a complex noise term that is Gaussian with a mean of 0 in the real and 
imaginary parts. The calibration problem then consists of estimating the complex antenna gains $\bmath{g}$ by
minimizing residuals in the LS sense:

\begin{equation}
\label{eq:cal:DI}
\min_{\bmath{g}}\sum_{pq}|r_{pq}|^2,~~~r_{pq}=d_{pq}-g_p m_{pq} \bar{g}_q, 
\end{equation}

where $d_{pq}$ are the observed visibilities. Treating this as a complex optimization problem as per the above, 
let us write out the complex Jacobian. 
We have a vector of $N$ complex unknowns $\bmath{g}$ and $M$ measured visibilities $d_{pq}$. It is conventional
to think of visibilities laid out in a visibility matrix; the normal approach at this stage is to vectorize $d_{pq}$ 
by fixing a numbering convention so as to enumerate all the possible antenna pairs $pq$ ($p<q$) using numbers from 1 to $M$.
Instead, let us keep using $pq$ as a single ``compound index'', with the implicit understanding that $pq$ in 
subscript corresponds to a single index from 1 to $M$ using \emph{some} fixed enumeration convention. 
Where necessary, we'll write $pq$ in square brackets (e.g. $a_{[pq],i}$) to emphasize this.

Now consider the patrial Jacobian $\JJ_k$ matrix in Eq.~\ref{eq:Jk}. This is of shape $M\times N$. We can write the
partial Jacobian in terms of its value at row $[pq]$ and column $j$ as 

\[
[ \JJ_k ]_{[pq],j} = \left \{  
  \begin{array}{ll} 
  -m_{pq}\bar{g}_q,& j=p, \\
  0, & \mathrm{otherwise.}
  \end{array}
\right .
\]

In other words, within each column $j$, $J_k$ is only non-zero at rows corresponding to baselines $[jq]$. We can express 
this more compactly using the Kronecker delta:


\[
J_k = - \overbrace{ \Matrix{c}{ m_{pq}\bar{g}_q\delta^{j}_p } } ^{j=1\dots N} \big \}{\scriptstyle [pq]=1\dots M~(p<q)}
\]

Likewise, the conjugate partial Jacobian $\JJ_{\bar{k}}$ may be written as

\[
J_{\bar{k}} = - \overbrace{ \Matrix{c}{ g_p m_{pq} \delta^{j}_q } } ^{j=1\dots N} \big \}{\scriptstyle [pq]=1\dots M~(p<q)}
\]

To give a specific example, for 3 antennas, using the numbering convention for $pq$ of
12, 13, 32, we have:

\[
\JJ_k = - \Matrix{ccc}{m_{12}\bar{g}_2 & 0 & 0 \\ m_{13}\bar{g}_3 & 0 & 0 \\ 0 & m_{23}\bar{g}_3 & 0 },
\JJ_{\bar{k}} = - \Matrix{ccc}{0 & g_1 m_{12} & 0 \\ 0 & 0 & g_1 m_{13} \\ 0 & 0 & g_2 m_{23} }
\]


The full complex Jacobian (Eq.~\ref{eq:JJ}) then becomes, in block matrix notation,

\begin{equation}
\label{eq:JJ:di:basic}
\begin{array}{r@{~}cc@{~}cc}
                & \overbrace{}^{j=1\dots N} & \overbrace{}^{j=1\dots N} \\
\JJ = - \bigg [ &
  \Stack{ m_{pq}\bar{g}_q\delta^{j}_p \\ \bar{m}_{pq} \bar{g}_p \delta^{j}_q } &
  \Stack{ g_p m_{pq} \delta^{j}_q \\ g_q \bar{m}_{pq} \delta^{j}_p }  
& \bigg ] &
\Stack{ \} {\scriptstyle [pq]=1\dots M}~(p<q) \\ \} {\scriptstyle [pq]=1\dots M}~(p<q) }

\end{array}
\end{equation}

where the $[pq]~(p<q)$ and $j$ subscripts within each block span the full range of $1\dots M$ and $1\dots N$. Now, 
since $d_{pq} = \bar{d}_{qp}$ and $m_{pq} = \bar{m}_{qp}$, we may notice
that the bottom half of the augumented residuals vector $\bmath{r}$ corresponds to the conjugate baselines 
$qp$ ($q>p$):


\[
\bmath{r} = \Matrix{c}{ r_{pq} \\ \bar{r}_{pq} } = \Matrix{c}{d_{pq}-g_p m_{pq}\bar{g}_q\\ \bar{d}_{pq}-\bar{g}_p \bar{m}_{pq}g_q} = 
\Matrix{c}{ r_{pq} \\ r_{qp} }~~ 
\Stack{ \} \scriptstyle [pq]=1\dots M~(p<q) \\ \} \scriptstyle [pq]=1\dots M~(p<q) }
\]

as does the bottom half of $\JJ$ in Eq.~\ref{eq:JJ:di:basic}. Note that we are free to reorder the rows of $\JJ$ and $\bmath{r}$ 
and intermix the normal and conjugate baselines, as this will not affect the LS equations derived at Eq.~\ref{eq:LM}.
This proves most convenient: instead of splitting $\JJ$ and $\bmath{r}$ into 
two vertical blocks with the compound index $[pq]~(p<q)$ running through $M$ rows within each block, we can treat 
the two blocks as one, with a single compound index $[pq]~(p\ne q)$ running through $2M$ rows:

\begin{equation}
\label{eq:JJ:di}
\begin{array}{r@{~}cc@{~}cc}
                & \overbrace{}^{j=1\dots N} & \overbrace{}^{j=1\dots N} \\
\JJ = - \big [ & m_{pq}\bar{g}_q\delta^{j}_p & g_p m_{pq} \delta^{j}_q & \big ],~
\bmath{r} = \big [ r_{pq} \big ] ~~{\} \scriptstyle [pq]=1\dots 2M~(p\ne q)}
\end{array}
\end{equation}

where for $q>p$, $r_{pq}=\bar{r}_{qp}$ and $m_{pq}=\bar{m}_{qp}$. For clarity, we may adopt the 
following order for enumerating the row index $[pq]$: $12,13,\dots,1n,$ $21,22,\dots,2n,$ $31,32,\dots,3n,$ $\dots,n1,\dots,n\,n-1$. 
For example, the complex Jacobian $\JJ$ for the 3-antenna case would then look like:

\[
-\Matrix{cccccc}{
  m_{12}\bar{g}_2 & 0               & 0 &  0          & g_1 m_{12} & 0           \\
  m_{13}\bar{g}_3 & 0               & 0 &  0          & 0          & g_1 m_{13}  \\
  0               & m_{21}\bar{g}_1 & 0 &  g_2 m_{21} & 0          & 0  \\
  0               & m_{23}\bar{g}_3 & 0 &  0          & 0          & g_2 m_{23} \\
  0               & 0               & m_{31}\bar{g}_1 & g_1 m_{31} & 0          & 0  \\
  0               & 0               & m_{32}\bar{g}_2 & 0 & g_3 m_{32} & 0 \\
}
\]


We can now write out the structure of $\JJ^H\JJ$. This is Hermitian, consisting of four $N\times N$ blocks:

\[
\JJ^H\JJ = \Matrix{cc}{\mat{A}&\mat{B}\\\mat{C}&\mat{D}} = \Matrix{cc}{\mat{A}&\mat{B}\\\mat{B}^H&\mat{A}}
\]

since the value at row $i$, column $j$ of each block is
\begin{eqnarray}
A_{ij} = \sum_{pq} \bar{m}_{pq} g_q m_{pq} \bar{g}_q \delta^{i}_p \delta^{j}_p &=& 
  \left \{ \begin{array}{cc}
        \sum_{q\ne i} |g_{q}^2| |m_{iq}^2|, & i=j \\
        0, & i\ne j
  \end{array} \right .\nonumber\\ 
B_{ij} = \sum_{pq} \bar{m}_{pq} g_q g_p m_{pq} \delta^{i}_p \delta^{j}_q &=& 
  \left \{ \begin{array}{cc}
      g_{i} g_{j} |m_{ij}^2|, & i\ne j\\
      0, & i=j
  \end{array} \right .\nonumber\\ 
C_{ij} = \sum_{pq} \bar{g}_p \bar{m}_{pq} m_{pq} \bar{g}_q \delta^{i}_q \delta^{j}_p &=& 
  \bar{B}_{ij} \nonumber\\
D_{ij} = \sum_{pq} \bar{g}_p \bar{m}_{pq} g_p m_{pq} \delta^{i}_q \delta^{j}_q &=& A_{ij} 
\label{eq:JHJ:DI:ABCD}
\end{eqnarray}

We then write $\JJ^H\JJ$ in terms of the four $N\times N$ blocks as:

\begin{equation}
\label{eq:JHJ:DI}
\JJ^H \JJ = \Matrix{c@{\bigg |}c}{
\mathrm{diag} \sum\limits_{q\ne i} |g^2_{q}| |m^2_{iq}| & 
  \left \{ 
  \begin{array}{@{}cc@{}}
   g_{i} g_{j} |m^2_{ij}|&{\scriptstyle i\ne j} \\
   0, &{\scriptstyle i=j}
  \end{array} \right .\\
  \left \{ 
  \begin{array}{@{}cc@{}}
   \bar{g}_{i} \bar{g}_{j} |m^2_{ij}|,&{\scriptstyle i\ne j} \\
   0, &{\scriptstyle i=j}
  \end{array} \right . 
  & \mathrm{diag} \sum\limits_{q\ne i} |g^2_{q}| |m^2_{iq}| 
}
% \Stack{ 
% \bigg \} \scriptstyle i=1\dots N \\ 
% \bigg \} \scriptstyle i=1\dots N 
%}
\end{equation}

For example, in the 3-antenna case, we have the following expression for
$\JJ^H \JJ$:

\newcommand{\yysq}[2]{{a^2_{#1}+a^2_{#2}}}
\newcommand{\bb}[1]{{b_{#1}}}
\newcommand{\bbb}[1]{{\bar{b}_{#1}}}
\[
{\scriptstyle
\Matrix{c@{}c@{}c@{}c@{}c@{}c}{
\yysq{12}{13} & 0             & 0             & 0             & \bb{12}       & \bb{13} \\
0             & \yysq{12}{23} & 0             & \bb{12}       & 0             & \bb{23} \\
0             & 0             & \yysq{13}{23} & \bb{13}       & \bb{23}       & 0       \\
0             & \bbb{12}      & \bbb{13}      & \yysq{12}{13} & 0             & 0       \\ 
\bbb{12}      & 0             & \bbb{23}      & 0             & \yysq{12}{23} & 0 \\
\bbb{13}      & \bbb{23}      & 0             & 0             & 0             &  \yysq{13}{23}  
}
},
\]
where $a_{pq}=|g_p|^2|m_{pq}|^2$, and $b_{pq}=g_p g_q |m_{pq}|^2$.

The other component of the LM equations (Eq.~\ref{eq:LM}) is the $\JJ^H\bmath{r}$ term. This will be a
$2N$-vector. We can write this as a stack of two $N$-vectors:


\begin{equation}
\label{eq:JHR:DI}
\JJ^H\bmath{r} = -\Matrix{c}{ 
\sum\limits_{pq} \bar{m}_{pq}g_q r_{pq} \delta^{i}_p  \\
\sum\limits_{pq} \bar{g}_p \bar{m}_{pq} r_{pq} \delta^{i}_q 
} = -\Matrix{c}{
\sum\limits_{q\ne i} \bar{m}_{iq}g_q r_{iq}   \\
\sum\limits_{q\ne i} \bar{g}_q m_{iq} \bar{r}_{iq}  
}
\Stack{ 
\big \} \scriptstyle i=1\dots N \\ 
\big \} \scriptstyle i=1\dots N,
}
\end{equation}

with the second equality established by swapping $p$ and $q$ in the bottom sum, and making use of $m_{pq}=\bar{m}_{qp}$ and $r_{pq}=\bar{r}_{qp}$. Clearly, the bottom vector is the conjugate of the top:

\begin{equation}
\label{eq:JHR:DI1}
\JJ^H\bmath{r} = -\Matrix{c}{\bmath{c}\\\bar{\bmath{c}}},~~c_i = \sum\limits_{q\ne i} \bar{m}_{iq} g_q r_{iq}.
\end{equation}

Equations~\ref{eq:JHJ:DI} and \ref{eq:JHR:DI1} give us the necessary ingredients for implementing complex
optimization of the RIME using LM, NG and/or steepest descent. 

\subsection{Approximating $\JJ^H\JJ$ and Stefcal}

The structure of $\JJ^H\JJ$ in Eq.~\ref{eq:JHJ:DI} suggests that it is diagonally dominant (especially 
for larger $N$). It is therefore not unreasonable to try approximating it with a diagonal matrix for purposes 
of inversion. Applying this to Eq.~\ref{eq:LM} with $\lambda=0$ (i.e. implementing NG minimization), we end up
with a simple per-antenna equation for computing the $\bmath{g}$ update step:

\begin{equation}
\delta g_p = \left( \sum\limits_{q\ne p} |g^2_{q}| |m^2_{pq}| \right )^{-1} \sum\limits_{q\ne p} \bar{m}_{pq} g_q r_{pq},
\end{equation}

or

\begin{equation}
\label{eq:JHJ:diag}
\delta g_p = \left( \sum\limits_{q\ne p} |y^2_{pq}| \right )^{-1} \sum\limits_{q\ne p} y_{pq} r_{pq},~~~y_{pq}=\bar{m}_{pq} g_q
\end{equation}

This is remarkably similar to the update step proposed by \citet{Stefcal} for the Stefcal algorithm, 
and by \citet{Mitchell-RTS} for MWA calibration. Note that these authors arrive at the equation from a different 
direction, by treating Eq.~\ref{eq:cal:DI} as a function of $\bmath{g}$ only, and ignoring the conjugate term. The 
resulting complex Jacobian (Eq.~\ref{eq:JJ:di:basic}) will then have null off-diagonal blocks, and $\JJ^H\JJ$ becomes 
diagonal.

Interestingly, if we use the diagonal $\JJ^H\JJ$ approximation with the modified LM method of Eq.~\ref{eq:LM1}, substitute 
$d_{pq} - g_p m_{pq} \bar{g}_q $ for $r_{pq}$ in the equations above, and compute the updated parameter vector as 
$\bmath{g}^\mathrm{(new)} = \bmath{g}+\delta \bmath{g}$, we arrive at


\begin{eqnarray*}
g_p^\mathrm{(new)} &=& \frac{\lambda}{1+\lambda}g_p + \frac{1}{1+\lambda} \frac{\sum\limits_{q\ne p} m_{pq}\bar{g}_q d_{pq}}{\sum\limits_{q\ne p} |g^2_{q}| |m^2_{pq}|}
\end{eqnarray*}

which for $\lambda=1$ essentially becomes the basic average-update step of Stefcal.

Establishing the equivalence between Stefcal and complex optimization with a diagonally-approximated $\JJ^H\JJ$ is 
very useful for our purposes, since the convergence properties of Stefcal have been thoroughly explored 
by \citet{Stefcal}, and we can therefore hope to apply these lessons here.

\subsection{Weighting}

Although \citet{ComplexOpt} do not mention this explicitly, it is straightforward to incorporate weights into the 
complex LS problem. Equation~\ref{eq:LSmin} is reformulated as

\begin{equation}
\label{eq:LSmin:w}
\min_{\bmath{z}} ||\mat{W} \bmath{r}(\bmath{z},\bmath{\bar z})||_F,
\end{equation}

where $\mat{W}$ is an $M\times M$ weights matrix (usually, the inverse of the data covariance matrix $\mat{C}$). This then propagates into the 
LM equations as

\begin{equation}
\label{eq:LM:W}
\Matrix{c}{\delta \zz \\ \delta \bar{\zz}} = -(\JJ^H \mat{W} \JJ + \lambda\II)^{-1}\JJ^H \mat{W} \bmath{r}_k,
\end{equation}

To use the diagonal $\JJ^H\JJ$ approximation, we must additionally assume that $\mat{W}$ is diagonal (i.e. that the noise terms
in Eq.~\ref{eq:RIME:unpol} are uncorrelated, which is normally the case for visibilities). We then have a simple weights vector
$w_{pq}$ (recall that we treat $pq$ as a compound index in this instance). The update equation then becomes:

\begin{equation}
\label{eq:JHJ:diag:W}
\delta g_p = \left( \sum\limits_{q\ne p} w_{pq} |y^2_{pq}| \right )^{-1} \sum\limits_{q\ne p} w_{pq} y_{pq} r_{pq},~~~y_{pq}=\bar{m}_{pq} g_q
\end{equation}

\subsection{Time/frequency solution intervals}

A very common use case is to measure multiple visibilities per each baseline $pq$, across a block of timeslots and frequency 
channels, then obtaining complex gain solutions that are constant across the block. The minimization problem can then
be written as

\begin{equation}
\label{eq:cal:DI:tf}
\min_{\bmath{g}}\sum_{pqt\nu}|d_{pqt\nu}-g_p m_{pqt\nu} \bar{g}_q|^2, 
\end{equation}

where $t$ and $\nu$ are timeslot and channel indices, respectively. We can repeat the derivations above using 
$[pqt\nu]$ as a single compound index. In deriving the $\JHJ$ term, the sums in Eq.~\ref{eq:JHJ:DI:ABCD}
must be taken over all $pqt\nu$ rather than just $pq$. We then have:

\begin{equation}
\label{eq:JHJ:DI:tf}
\JJ^H \JJ = \Matrix{c@{\bigg |}c}{
\mathrm{diag} \sum\limits_{t\nu,q\ne i} |g^2_{q}| |m^2_{iqt\nu}| & 
  \left \{ 
  \begin{array}{@{}cc@{}}
   \sum\limits_{t\nu} g_{i} g_{j} |m^2_{ijt\nu}|&{\scriptstyle i\ne j} \\
   0, &{\scriptstyle i=j}
  \end{array} \right .\\
  \left \{ 
  \begin{array}{@{}cc@{}}
   \sum\limits_{t\nu} \bar{g}_{i} \bar{g}_{j} |m^2_{ijt\nu}|,&{\scriptstyle i\ne j} \\
   0, &{\scriptstyle i=j}
  \end{array} \right . 
  & \mathrm{diag} \sum\limits_{t\nu,q\ne i} |g^2_{q}| |m^2_{iq}| 
}
% \Stack{ 
% \bigg \} \scriptstyle i=1\dots N \\ 
% \bigg \} \scriptstyle i=1\dots N 
%}
\end{equation}

and 

\begin{equation}
\label{eq:JHR:DI:tf}
\JJ^H\bmath{r} 
= -\Matrix{c}{
\sum\limits_{t\nu,q\ne i} m_{iqt\nu}\bar{g}_q r_{iqt\nu}   \\
\sum\limits_{t\nu,q\ne i} g_q \bar{m}_{iqt\nu} \bar{r}_{iqt\nu}  
}
\end{equation}

Using the diagonal approximation (and adding weights), we then have the following update step:

\begin{eqnarray*}
\label{eq:JHJ:diag:W:tf}
\delta g_p &=& \left( \sum\limits_{t\nu,q\ne p} w_{pqt\nu} |y^2_{pqt\nu}| \right )^{-1} \sum\limits_{t\nu,q\ne p} w_{pqt\nu} y_{pqt\nu} r_{pqt\nu}\\
y_{pqt\nu}&=&\bar{m}_{pqt\nu}g_q \nonumber
\end{eqnarray*}


\subsection{Time/frequency sliding windows}

The latter formulation allows for a particularly elegant way of implementing gain solutions per timeslot, per channel
that are constrained to be smooth over frequency and time. To see this, let's consider the visibilities and gains as
continuous functions of frequency and time $m_{pq}(t,\nu)$, $d_{pq}(t,\nu)$ and $g_p(t,\nu)$ (in practice, these will be sampled over some kind of frequency and time grid). Then, we may choose a two-dimensional windowing function $w(t,\nu)$ -- for example, 
a Gaussian smoothing kernel:

\[
w(x,y) = \frac{1}{2\pi\sqrt{\sigma_x\sigma_y}} \exp\left( -\frac{x^2}{2\sigma^2_x} - \frac{y^2}{2\sigma_y^2} \right),
\]

and replace summation over $tv$ in the update step above with a convolution:

\begin{equation}
\label{eq:JHJ:diag:smooth}
\delta g_p = \left( w\circ\sum\limits_{q\ne p} |y^2_{pq}| \right )^{-1} \left( w\circ \sum\limits_{q\ne p} y_{pq} r_{pq} \right )\\
\end{equation}

Iterating the above to convergence, we obtain a gain solution $\bmath{g}(t,\nu)$ that is smooth in frequency and time, with the degree of smoothness 
constrained by the windowing function. Note that for any given fixed time and frequency $t_0,\nu_0$, this is equivalent to using Stefcal to solve a weighted least-squares version of Eq.~\ref{eq:LSmin:w}, using a ``sliding window'' of weights centred on $t_0,\nu_0$, given by $w(t-t_0,\nu-\nu_0)$.

\section{Polarized direction-independent calibration}

To incorporate polarization, let us start by rewriting the basic RIME of Eq.~\ref{eq:RIME:unpol} using $2\times 2$ matrices \citep[a full derivation
may be found in][]{RRIME1}:

\begin{equation}
\label{eq:RIME:pol}
\DD_{pq} = \GG_p \MM_{pq} \GG^H_q + \mat{N}_{pq}.
\end{equation}

Here, $\DD_{pq}$ is the \emph{visibility matrix} observed by baseline $pq$, $\MM_{pq}$ is the sky \emph{coherency matrix}, $\GG_p$ is the Jones matrix associated with antenna $p$, and $\mat{N}_{pq}$ is a noise matrix. Quite importantly, the visibility and coherency matrices are Hermitian: 
$\DD_{pq}=\DD^H_{qp}$, and $\MM_{pq}=\MM^H_{qp}$. The basic polarization calibration problem can be formulated as

\begin{equation}
\label{eq:cal:DI:pol}
\min_{\{\GG_p\}}\sum_{pq}||\RR_{pq}||_F,~~
\RR_{pq} = \DD_{pq}-\GG_p \MM_{pq} \GG^H_q
\end{equation}


\newcommand{\Rop}[1]{\mathcal{R}_{{#1}}}
\newcommand{\Lop}[1]{\mathcal{L}_{{#1}}}
\newcommand{\Top}{\mathcal{T}}

In principle, this can be recast into an ordinary complex NLLS problem (Eq.~\ref{eq:LSmin}) by vectorizing each matrix in the above equation, 
and then deriving the complex Jacobian according to Eq.~\ref{eq:JJ}. This is somewhat cumbersome, but we can obtain a more elegant derivation
by using $2\times2$ matrices instead of scalars, and writing the Jacobian in terms of linear operators on matrices. Appendix~\ref{sec:4x4app}
will establish that this is entirely equivalent to the conventional (vectorized) approach. 

For starters, let us define two linear operators acting on $2\times2$ matrices: the ``right-multiply'' and ``left-multiply by $\mat{A}$'' \
operators, where $\mat{A}$ is a given $2\times2$ matrix:

\begin{equation}
\label{eq:def::RL}
\Rop{\mat{A}} [\mat{X}] = \mat{XA}, ~~~~~
\Lop{\mat{A}} [\mat{X}] = \mat{AX}.
\end{equation}

Trivial properties of these two operators are 

\begin{eqnarray}
\label{eq:LA:RATT}
\Lop{\mat{A}}[\cdot] = \big[ \Rop{\mat{A}^T}[\cdot] \big ]^T~~&~&~~
\big[ \Lop{\mat{A}}[\cdot] \big]^T = \Rop{\mat{A}^T}[\cdot] \nonumber\\
\Lop{\bar{\mat{A}}}[\cdot] = \big[ \Rop{\mat{A}^H}[\cdot] \big ]^T~~&~&~~
\big[ \Lop{\bar{\mat{A}}}[\cdot] \big]^T = \Rop{\mat{A}^H}[\cdot]
\end{eqnarray}

Next, let us define two operators that take the Wirtinger derivatives w.r.t. a complex matrix:

\begin{equation}
\frac{\partial}{\partial\GG} = \Matrix{cc}{ {\partial}/{\partial g_{11}} & {\partial}/{\partial g_{12}} \\
{\partial}/{\partial g_{21}} & {\partial}/{\partial g_{22}}} \\
\frac{\partial}{\partial{\GG^H}} = \Matrix{cc}{ {\partial}/{\partial \bar{g}_{11}} & {\partial}/{\partial \bar{g}_{21}} \\
{\partial}/{\partial \bar{g}_{12}} & {\partial}/{\partial \bar{g}_{22}}}
\end{equation}

From Eq.~\ref{eq:cal:DI:pol}, one can see that

\[
\frac{\partial\RR_{pq}}{\partial\GG_p} = -\Rop{\MM_{pq}\GG^H_q},\mathrm{~~and~~}
\frac{\partial\RR_{pq}}{\partial\GG^H_q} = -\Lop{\GG_p \MM_{pq}}.
\]

If we now stack the gain matrices of Eq.~\ref{eq:cal:DI:pol} into one column ``vector of matrices'' 

\[
\GG = [ \GG_1,\dots,\GG_N,\GG^H_1,\dots,\GG^H_N ]^T,
\]

then we may construct the top half of the full complex Jacobian operator in full analogy with the 
derivation of Eq.~\ref{eq:JJ:di:basic}. We'll use the same ``compound index'' convention for $pq$. That is, 
$[pq]$ will represent a single index running through $M$ values (i.e. enumerating all combinations of $p<q$).

\[
\JJ_\mathrm{top} = - \Matrix{cc}{ 
\Rop{ \MM_{pq}\GG^H_q }\delta^j_p & 
\Lop{ \GG_p \MM_{pq}  }\delta^j_q 
} ~~ \} \scriptstyle [pq]=1\dots M~(p<q)
% 
%\Lop{ \bar{\GG}_p    \bar{\MM}_{pq} }\delta^j_q & 
%\Rop{ \bar{\MM}_{pq} \bar{\GG^H_q}  }\delta^j_p  
%}.
\]

Consider now the bottom half of the Jacobian. In Eq.~\ref{eq:JJ}, this corresponds to the complex conjugate of the residuals, 
and is constructed by conjugating and mirroring the top half. Let us modify this construction by taking the Hermitian transpose 
of the residuals instead:

\[
\bmath{r} = 
\Matrix{c}{
  \RR_{pq} \\ 
  \RR^H_{pq} 
} = 
\Matrix{c}{
  \RR_{pq} \\ 
  {[ \bar{\RR}_{pq} ]}^T
} 
~~ 
\Stack{ 
\} \scriptstyle [pq]=1\dots M~(p<q) \\ 
\} \scriptstyle [pq]=1\dots M~(p<q) 
}
\]

This adds an extra transpose operation over the usual definition which must then be propagated\footnote{As shown in Appendix~\ref{sec:4x4app}, the transpose is simply a matter of reordering rows in the Jacobian and the augmented residual vector.} into the bottom half of the Jacobian:

\[
\JJ = - \Matrix{cc}{ 
\Rop{ \MM_{pq}\GG^H_q }\delta^j_p & 
\Lop{ \GG_p \MM_{pq}  }\delta^j_q \\
{[ \Lop{ \bar{\GG}_p    \bar{\MM}_{pq} } ]}^T\delta^j_q & 
{[ \Rop{ \bar{\MM}_{pq} \bar{\GG^H_q}  } ]}^T\delta^j_p  
}
\]

which, making use of Eq.~\ref{eq:LA:RATT}, becomes simply:

\[
\JJ = - \Matrix{cc}{ 
\Rop{ \MM_{pq}\GG^H_q }\delta^j_p & 
\Lop{ \GG_p \MM_{pq}  }\delta^j_q \\
\Rop{ \MM^H_{pq} \GG^H_p } \delta^j_q & 
\Lop{ \GG_q \MM^H_{pq}  } \delta^j_p  
}~~ 
\Stack{ 
\} \scriptstyle [pq]=1\dots M~(p<q) \\ 
\} \scriptstyle [pq]=1\dots M~(p<q) 
}
\]

We may now make exactly the same observation as we did to derive Eq.~\ref{eq:JJ:di}, and rewrite both $\JJ$ and $\bmath{r}$ in terms of 
a single row block. The $pq$ index will now run through $2M$ values (i.e. enumerating all combinations of $p\ne q$):

\begin{equation}
\JJ = - \Matrix{cc}{ 
\Rop{ \MM_{pq}\GG^H_q }\delta^j_p & 
\Lop{ \GG_p \MM_{pq}  }\delta^j_q \\
}~~ 
\} \scriptstyle [pq]=1\dots 2M~(p\ne q)\\ 
\end{equation}

and 

\begin{equation}
\bmath{r} = \Matrix{c}{ \mat{R}_{pq}}~~ 
\} \scriptstyle [pq]=1\dots 2M~(p\ne q)\\ 
\end{equation}




\bibliographystyle{mn2e}
\bibliography{cjpaper}

\appendix

\section{Deriving $4\times4$ operator representations}
\label{sec:4x4app}

If we introduce the vectorization operator ``vec'' in the usual way:

\newcommand{\VEC}[1]{\mathrm{vec}\,{#1}}

\[
\VEC{\mat{X}} = \Matrix{c}{x_{11}\\x_{12}\\x_{21}\\x_{22}},
\]

then any linear operator on $\mat{X}$ must be equivalent to multiplication of $\VEC{\mat{X}}$ by a patricular $4\times 4$ matrix. We will leave the derivation of these matrixes to the appendix, for our purposes it is sufficient to know that these matrices exist. We'll reuse the notation 
$\Rop{\mat{A}}$ and $\Lop{\mat{A}}$ to represent the $4\times4$ matrices as well. Finally, note that

Indeed, if $\mat{Y}=\Rop{\mat{A}}[\mat{X}]$, then 

\begin{equation}
\Matrix{c}{y_{11}\\y_{12}\\y_{21}\\y_{22}} = \VEC{\Rop{\mat{A}}[\mat{X}]} = 
\Matrix{cccc}{a_{11}&a_{21}&0&0 \\ a_{12}&a_{22}&0&0 \\ 0&0&a_{11}&a_{21} \\ 0&0&a_{12}&a_{22} }
\Matrix{c}{x_{11}\\x_{12}\\x_{21}\\x_{22}} 
\end{equation}

Likewise, 

\begin{equation}
\VEC{\Lop{\mat{A}}[\mat{X}]} = 
\Matrix{cccc}{a_{11}&a_{12}&0&0 \\ 0&0&a_{11}&a_{12} \\ a_{21}&a_{22}&0&0  \\ 0&0&a_{21}&a_{22} }
\VEC{\mat{X}},
\end{equation}

and

\begin{equation}
\VEC{\Top[\mat{X}]} = 
\Matrix{cccc}{1&0&0&0 \\ 0&0&1&0 \\ 0&1&0&0 \\ 0&0&0&1} \VEC{\mat{X}}.
\end{equation}


We'll reuse the notation $\Top$, $\Rop{\mat{A}}$ and $\Lop{\mat{A}}$ to represent the $4\times4$ matrices above. Now, note that the
three operators have an important relationship

\begin{equation}
%\label{eq:LA:TRA}
\Lop{\mat{A}^T}[\mat{X}] = \Top \big[ \Rop{\mat{A}}[\mat{X}] \big ]
\end{equation}

which is trivially derived from their definitions. This must necessarily also hold for the corresponding $4\times4$ matrices:
$\Lop{\mat{A}^T} = \Top \Rop{\mat{A}}$, and can be verified from the matrix definitions above.


\label{lastpage}

\end{document}